{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9174f3a5",
   "metadata": {},
   "source": [
    "# Headers and MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "338515b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac76122",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5d75d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = pd.read_csv('/home/azureuser/cloudfiles/code/Users/vlavanga/data/processed/snowflake_v5_pre_ts.csv')\n",
    "df_main = df_main.drop(columns=['SALES_ORG','COUNTRY','FABRIC_CONTENT_CODE_TEXT'])\n",
    "df_main = df_main[df_main['SILHOUETTE'].isin(['5950','LP5950', '950', '3930','940','920'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c8ef53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PO_CREATED_DATE</th>\n",
       "      <th>REGION</th>\n",
       "      <th>SALES_ORG_NAME</th>\n",
       "      <th>FABRIC_TYPE</th>\n",
       "      <th>TEAM</th>\n",
       "      <th>SILHOUETTE</th>\n",
       "      <th>SPORT</th>\n",
       "      <th>DIVISION_NAME</th>\n",
       "      <th>SEASON_CONSOLIDATION</th>\n",
       "      <th>ORDERED_QUANTITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20231003</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>YOMIURI GIANTS</td>\n",
       "      <td>LP5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Custom</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20231003</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>YOMIURI GIANTS</td>\n",
       "      <td>LP5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Custom</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20231109</td>\n",
       "      <td>Emerging Markets</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>LAS VEGAS RAIDERS</td>\n",
       "      <td>940</td>\n",
       "      <td>FOOTBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Program</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20231103</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>RANCHO CUCAMONGA QUAKES</td>\n",
       "      <td>5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Program</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20231103</td>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>RENO ACES</td>\n",
       "      <td>5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Program</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PO_CREATED_DATE            REGION SALES_ORG_NAME FABRIC_TYPE  \\\n",
       "0         20231003             Japan          Japan      Wovens   \n",
       "1         20231003             Japan          Japan      Wovens   \n",
       "2         20231109  Emerging Markets    New Zealand      Wovens   \n",
       "3         20231103     North America  United States      Wovens   \n",
       "4         20231103     North America  United States      Wovens   \n",
       "\n",
       "                      TEAM SILHOUETTE     SPORT DIVISION_NAME  \\\n",
       "0           YOMIURI GIANTS     LP5950  BASEBALL      Headwear   \n",
       "1           YOMIURI GIANTS     LP5950  BASEBALL      Headwear   \n",
       "2        LAS VEGAS RAIDERS        940  FOOTBALL      Headwear   \n",
       "3  RANCHO CUCAMONGA QUAKES       5950  BASEBALL      Headwear   \n",
       "4                RENO ACES       5950  BASEBALL      Headwear   \n",
       "\n",
       "  SEASON_CONSOLIDATION  ORDERED_QUANTITY  \n",
       "0               Custom                72  \n",
       "1               Custom                72  \n",
       "2              Program                80  \n",
       "3              Program                 1  \n",
       "4              Program                 1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d608b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaT count after conversion: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert to string\n",
    "df_main[\"PO_CREATED_DATE\"] = df_main[\"PO_CREATED_DATE\"].astype(str)\n",
    "\n",
    "# Step 2: Convert to datetime\n",
    "df_main[\"PO_CREATED_DATE\"] = pd.to_datetime(\n",
    "    df_main[\"PO_CREATED_DATE\"],\n",
    "    format=\"%Y%m%d\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Step 3: Check if any NaT created\n",
    "print(\"NaT count after conversion:\", df_main[\"PO_CREATED_DATE\"].isna().sum())\n",
    "\n",
    "# Step 4: Filter from 2009 onwards\n",
    "df_main = df_main[df_main[\"PO_CREATED_DATE\"] >= \"2009-01-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b855562",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['TEAM'] = df_main['TEAM'].fillna('Unknown')\n",
    "df_main['SILHOUETTE'] = df_main['SILHOUETTE'].fillna('Unknown')\n",
    "df_main['SPORT'] = df_main['SPORT'].fillna('Unknown')\n",
    "df_main['SALES_ORG_NAME'] = df_main['SALES_ORG_NAME'].fillna('Unknown')\n",
    "\n",
    "# Convert date â†’ MonthTimestamp \n",
    "df_main['DATE'] = df_main['PO_CREATED_DATE'].dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Sort for proper lag creation\n",
    "df_main = df_main.sort_values('DATE')\n",
    "df_main.drop(['PO_CREATED_DATE'], axis=1, inplace=True)\n",
    "df = df_main.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce2c158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NONE', 'BASEBALL', 'COLLEGE', 'GOLF', 'SOCCER', 'HOCKEY',\n",
       "       'BASKETBALL', 'ENTERTAINMENT', 'FOOTBALL', 'RACING',\n",
       "       'COBRANDED CORE', 'RUGBY', 'CRICKET', 'LACROSSE',\n",
       "       'COBRANDED BASEBALL', 'COBRANDED RACING', 'ANIMATED CHARACTER',\n",
       "       'COBRANDED FOOTBALL', 'COBRANDED BASKETBALL', 'WATER SPORTS',\n",
       "       'CYCLING', 'COBRANDED COLLEGE', 'VOLLEYBALL', 'BOXING',\n",
       "       'WRESTLING', 'HANDBALL', 'TENNIS', 'E SPORT', 'GAELIC FOOTBALL',\n",
       "       'COBRANDED NON LICENSED', 'SOFTBALL', 'COBRANDED HOCKEY',\n",
       "       'COBRANDED SOCCER', 'SNOWSPORTS'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SPORT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75862230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================= REGION =================\n",
      "Unique Count: 4\n",
      "['North America' 'EMEA' 'Emerging Markets' 'Japan']\n",
      "\n",
      "================= SALES_ORG_NAME =================\n",
      "Unique Count: 14\n",
      "['United States' 'EMEA' 'Canada' 'Latin America' 'Southeast Asia'\n",
      " 'Australia' 'South Korea' 'Mexico' 'German' 'New Zealand' 'Japan' 'China'\n",
      " 'NEC China Shanghai' 'US Retail']\n",
      "\n",
      "================= FABRIC_TYPE =================\n",
      "Unique Count: 4\n",
      "['Wovens' 'Other' 'Knits' 'Polyester']\n",
      "\n",
      "================= TEAM =================\n",
      "Unique Count: 3605\n",
      "['NONE' 'NEW YORK YANKEES' 'WBC CANADA' ... 'MENTOS'\n",
      " 'NHL WINTER CLASSICS LOGO' 'WHITE MOUNTAINEERING']\n",
      "\n",
      "================= SILHOUETTE =================\n",
      "Unique Count: 6\n",
      "['5950' '940' '3930' '920' 'LP5950' '950']\n",
      "\n",
      "================= SPORT =================\n",
      "Unique Count: 34\n",
      "['NONE' 'BASEBALL' 'COLLEGE' 'GOLF' 'SOCCER' 'HOCKEY' 'BASKETBALL'\n",
      " 'ENTERTAINMENT' 'FOOTBALL' 'RACING' 'COBRANDED CORE' 'RUGBY' 'CRICKET'\n",
      " 'LACROSSE' 'COBRANDED BASEBALL' 'COBRANDED RACING' 'ANIMATED CHARACTER'\n",
      " 'COBRANDED FOOTBALL' 'COBRANDED BASKETBALL' 'WATER SPORTS' 'CYCLING'\n",
      " 'COBRANDED COLLEGE' 'VOLLEYBALL' 'BOXING' 'WRESTLING' 'HANDBALL' 'TENNIS'\n",
      " 'E SPORT' 'GAELIC FOOTBALL' 'COBRANDED NON LICENSED' 'SOFTBALL'\n",
      " 'COBRANDED HOCKEY' 'COBRANDED SOCCER' 'SNOWSPORTS']\n",
      "\n",
      "================= DIVISION_NAME =================\n",
      "Unique Count: 2\n",
      "['Headwear' 'Apparel']\n",
      "\n",
      "================= SEASON_CONSOLIDATION =================\n",
      "Unique Count: 3\n",
      "['Custom' 'Program' 'Stock']\n"
     ]
    }
   ],
   "source": [
    "# Inspect unique values for all relevant categorical columns\n",
    "cols_to_check = [\n",
    "    \"REGION\",\n",
    "    \"SALES_ORG_NAME\",\n",
    "    \"FABRIC_TYPE\",\n",
    "    \"TEAM\",\n",
    "    \"SILHOUETTE\",\n",
    "    \"SPORT\",\n",
    "    \"DIVISION_NAME\",\n",
    "    \"SEASON_CONSOLIDATION\"\n",
    "]\n",
    "\n",
    "for col in cols_to_check:\n",
    "    print(f\"\\n================= {col} =================\")\n",
    "    print(\"Unique Count:\", df_main[col].nunique())\n",
    "    print(df_main[col].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86ad42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped monthly shape: (16188, 14)\n"
     ]
    }
   ],
   "source": [
    "# Final grouping keys (define the time series)\n",
    "group_keys = [\"SALES_ORG_NAME\", \"SILHOUETTE\", \"SEASON_CONSOLIDATION\", \"DATE\"]\n",
    "\n",
    "df_grouped = (\n",
    "    df.groupby(group_keys)\n",
    "        .agg(\n",
    "            ORDERED_QUANTITY=('ORDERED_QUANTITY', 'sum'),\n",
    "\n",
    "            # Aggregated features we want for TFT\n",
    "            TEAM_COUNT    = ('TEAM',    pd.Series.nunique),\n",
    "            SPORT_COUNT   = ('SPORT',   pd.Series.nunique),\n",
    "            FABRIC_COUNT  = ('FABRIC_TYPE', pd.Series.nunique),\n",
    "            DIVISION_COUNT= ('DIVISION_NAME', pd.Series.nunique),\n",
    "\n",
    "            # First-category assignment within that month (stable)\n",
    "            REGION        = ('REGION', lambda x: x.iloc[0]),\n",
    "            TEAM          = ('TEAM',   lambda x: x.iloc[0]),\n",
    "            SPORT         = ('SPORT',  lambda x: x.iloc[0]),\n",
    "            FABRIC_TYPE   = ('FABRIC_TYPE', lambda x: x.iloc[0]),\n",
    "            DIVISION_NAME = ('DIVISION_NAME', lambda x: x.iloc[0])\n",
    "        )\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Grouped monthly shape:\", df_grouped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1ff1bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns: ['REGION', 'SALES_ORG_NAME', 'FABRIC_TYPE', 'TEAM', 'SILHOUETTE', 'SPORT', 'DIVISION_NAME', 'SEASON_CONSOLIDATION', 'ORDERED_QUANTITY', 'DATE']\n",
      "df_selected shape: (6076376, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGION</th>\n",
       "      <th>SALES_ORG_NAME</th>\n",
       "      <th>FABRIC_TYPE</th>\n",
       "      <th>TEAM</th>\n",
       "      <th>SILHOUETTE</th>\n",
       "      <th>SPORT</th>\n",
       "      <th>DIVISION_NAME</th>\n",
       "      <th>SEASON_CONSOLIDATION</th>\n",
       "      <th>ORDERED_QUANTITY</th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>614819</th>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>NONE</td>\n",
       "      <td>5950</td>\n",
       "      <td>NONE</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Custom</td>\n",
       "      <td>1028</td>\n",
       "      <td>2009-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1059455</th>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>NEW YORK YANKEES</td>\n",
       "      <td>5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Custom</td>\n",
       "      <td>30</td>\n",
       "      <td>2009-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4665257</th>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>NEW YORK YANKEES</td>\n",
       "      <td>5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Custom</td>\n",
       "      <td>30</td>\n",
       "      <td>2009-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2743571</th>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>WBC CANADA</td>\n",
       "      <td>940</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Custom</td>\n",
       "      <td>30</td>\n",
       "      <td>2009-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871709</th>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>HOUSTON ASTROS</td>\n",
       "      <td>5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Custom</td>\n",
       "      <td>30</td>\n",
       "      <td>2009-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                REGION SALES_ORG_NAME FABRIC_TYPE              TEAM  \\\n",
       "614819   North America  United States      Wovens              NONE   \n",
       "1059455  North America  United States      Wovens  NEW YORK YANKEES   \n",
       "4665257  North America  United States      Wovens  NEW YORK YANKEES   \n",
       "2743571  North America  United States      Wovens        WBC CANADA   \n",
       "1871709  North America  United States      Wovens    HOUSTON ASTROS   \n",
       "\n",
       "        SILHOUETTE     SPORT DIVISION_NAME SEASON_CONSOLIDATION  \\\n",
       "614819        5950      NONE      Headwear               Custom   \n",
       "1059455       5950  BASEBALL      Headwear               Custom   \n",
       "4665257       5950  BASEBALL      Headwear               Custom   \n",
       "2743571        940  BASEBALL      Headwear               Custom   \n",
       "1871709       5950  BASEBALL      Headwear               Custom   \n",
       "\n",
       "         ORDERED_QUANTITY       DATE  \n",
       "614819               1028 2009-01-01  \n",
       "1059455                30 2009-01-01  \n",
       "4665257                30 2009-01-01  \n",
       "2743571                30 2009-01-01  \n",
       "1871709                30 2009-01-01  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select only the desired columns (use cols_to_check and keep ORDERED_QUANTITY & DATE if present)\n",
    "selected_cols = [c for c in cols_to_check if c in df.columns]\n",
    "for extra in [\"ORDERED_QUANTITY\", \"DATE\"]:\n",
    "    if extra in df.columns and extra not in selected_cols:\n",
    "        selected_cols.append(extra)\n",
    "\n",
    "df_selected = df[selected_cols].copy()\n",
    "\n",
    "print(\"Selected columns:\", selected_cols)\n",
    "print(\"df_selected shape:\", df_selected.shape)\n",
    "df_selected.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806075b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected = df_selected.drop(columns=['REGION',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49308fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"SALES_ORG_NAME\", \"SILHOUETTE\", \"SEASON_CONSOLIDATION\", \"DATE\"]\n",
    "except_cols = ['REGION','TEAM','SPORT','DIVISION_NAME']\n",
    "missing_dates_df_final, pairs_final = verify_grp_timestamps(df_selected,cat_cols,except_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9200b06b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Year</th>\n",
       "      <th>Missing Months</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(Australia, 3930, Custom, 2011-08-01 00:00:00)</td>\n",
       "      <td>2011</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Australia, 3930, Custom, 2011-09-01 00:00:00)</td>\n",
       "      <td>2011</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(Australia, 3930, Custom, 2011-10-01 00:00:00)</td>\n",
       "      <td>2011</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Australia, 3930, Custom, 2011-11-01 00:00:00)</td>\n",
       "      <td>2011</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Australia, 3930, Custom, 2012-05-01 00:00:00)</td>\n",
       "      <td>2012</td>\n",
       "      <td>[1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16183</th>\n",
       "      <td>(United States, LP5950, Stock, 2025-01-01 00:0...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16184</th>\n",
       "      <td>(United States, LP5950, Stock, 2025-04-01 00:0...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16185</th>\n",
       "      <td>(United States, LP5950, Stock, 2025-05-01 00:0...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16186</th>\n",
       "      <td>(United States, LP5950, Stock, 2025-06-01 00:0...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16187</th>\n",
       "      <td>(United States, LP5950, Stock, 2025-09-01 00:0...</td>\n",
       "      <td>2025</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16188 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Group  Year  \\\n",
       "0         (Australia, 3930, Custom, 2011-08-01 00:00:00)  2011   \n",
       "1         (Australia, 3930, Custom, 2011-09-01 00:00:00)  2011   \n",
       "2         (Australia, 3930, Custom, 2011-10-01 00:00:00)  2011   \n",
       "3         (Australia, 3930, Custom, 2011-11-01 00:00:00)  2011   \n",
       "4         (Australia, 3930, Custom, 2012-05-01 00:00:00)  2012   \n",
       "...                                                  ...   ...   \n",
       "16183  (United States, LP5950, Stock, 2025-01-01 00:0...  2025   \n",
       "16184  (United States, LP5950, Stock, 2025-04-01 00:0...  2025   \n",
       "16185  (United States, LP5950, Stock, 2025-05-01 00:0...  2025   \n",
       "16186  (United States, LP5950, Stock, 2025-06-01 00:0...  2025   \n",
       "16187  (United States, LP5950, Stock, 2025-09-01 00:0...  2025   \n",
       "\n",
       "                             Missing Months  \n",
       "0      [1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12]  \n",
       "1      [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12]  \n",
       "2       [1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12]  \n",
       "3       [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12]  \n",
       "4      [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12]  \n",
       "...                                     ...  \n",
       "16183  [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]  \n",
       "16184  [1, 2, 3, 5, 6, 7, 8, 9, 10, 11, 12]  \n",
       "16185  [1, 2, 3, 4, 6, 7, 8, 9, 10, 11, 12]  \n",
       "16186  [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12]  \n",
       "16187  [1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12]  \n",
       "\n",
       "[16188 rows x 3 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_dates_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "140c2cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After month filling: (27668, 14)\n"
     ]
    }
   ],
   "source": [
    "def fill_missing_months(df, keys):\n",
    "    out = []\n",
    "    for name, g in df.groupby(keys):\n",
    "\n",
    "        g = g.sort_values(\"MONTH\")\n",
    "\n",
    "        full_range = pd.date_range(\n",
    "            start=g[\"MONTH\"].min(),\n",
    "            end=g[\"MONTH\"].max(),\n",
    "            freq=\"MS\"\n",
    "        )\n",
    "\n",
    "        g2 = g.set_index(\"MONTH\").reindex(full_range)\n",
    "        g2.index.name = \"MONTH\"\n",
    "\n",
    "        # Numerical fill\n",
    "        num_cols = [\"ORDERED_QUANTITY\",\"TEAM_COUNT\",\"SPORT_COUNT\",\"FABRIC_COUNT\",\"DIVISION_COUNT\"]\n",
    "        g2[num_cols] = g2[num_cols].fillna(0)\n",
    "\n",
    "        # Categorical forward-fill\n",
    "        cat_cols = [\"REGION\",\"TEAM\",\"SPORT\",\"FABRIC_TYPE\",\"DIVISION_NAME\"]\n",
    "        for c in cat_cols:\n",
    "            g2[c] = g2[c].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "        # Restore group_id values\n",
    "        for i, k in enumerate(keys):\n",
    "            g2[k] = name[i]\n",
    "\n",
    "        out.append(g2.reset_index())\n",
    "\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "df_filled = fill_missing_months(df_grouped, \n",
    "                                keys=[\"SALES_ORG_NAME\", \"SILHOUETTE\", \"SEASON_CONSOLIDATION\"])\n",
    "\n",
    "print(\"After month filling:\", df_filled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d9d7104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled['year'] = df_filled['MONTH'].dt.year\n",
    "df_filled['month_num'] = df_filled['MONTH'].dt.month\n",
    "df_filled['quarter'] = df_filled['MONTH'].dt.quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ddb1f0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train has time_idx: False\n",
      "Test has time_idx: False\n"
     ]
    }
   ],
   "source": [
    "test_start = pd.Timestamp(\"2024-01-01\")\n",
    "\n",
    "train_df = df_filled[df_filled[\"MONTH\"] < test_start].copy()\n",
    "test_df  = df_filled[df_filled[\"MONTH\"] >= test_start].copy()\n",
    "\n",
    "print(\"Train has time_idx:\", \"time_idx\" in train_df.columns)\n",
    "print(\"Test has time_idx:\", \"time_idx\" in test_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49310d",
   "metadata": {},
   "source": [
    "# Model TFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9962d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_and_set_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "\n",
    "    print(f\"\\nðŸ”’ Training with SEED: {seed}\\n\")\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    return seed\n",
    "\n",
    "\n",
    "df_main = pd.read_csv(\n",
    "    \"/home/azureuser/cloudfiles/code/Users/vlavanga/data/processed/snowflake_v5_pre_ts.csv\",\n",
    ")\n",
    "\n",
    "df_main = df_main.drop(columns=['SALES_ORG','COUNTRY','FABRIC_CONTENT_CODE_TEXT'], errors='ignore')\n",
    "\n",
    "valid_sil = ['5950','LP5950', '950', '3930','940','920']\n",
    "df_main = df_main[df_main['SILHOUETTE'].isin(valid_sil)]\n",
    "\n",
    "\n",
    "fill_cols = [\"TEAM\", \"SPORT\", \"SILHOUETTE\", \"SALES_ORG_NAME\", \n",
    "             \"REGION\", \"DIVISION_NAME\", \"FABRIC_TYPE\"]\n",
    "\n",
    "for col in fill_cols:\n",
    "    df_main[col] = df_main[col].fillna(\"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "de669e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main['PO_CREATED_DATE'] = pd.to_datetime(df_main['PO_CREATED_DATE'], format='%Y%m%d')\n",
    "\n",
    "df_main[\"MONTH\"] = df_main[\"PO_CREATED_DATE\"].dt.to_period(\"M\").dt.to_timestamp()\n",
    "df_main = df_main.drop(columns=[\"PO_CREATED_DATE\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ea74eccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REGION</th>\n",
       "      <th>SALES_ORG_NAME</th>\n",
       "      <th>FABRIC_TYPE</th>\n",
       "      <th>TEAM</th>\n",
       "      <th>SILHOUETTE</th>\n",
       "      <th>SPORT</th>\n",
       "      <th>DIVISION_NAME</th>\n",
       "      <th>SEASON_CONSOLIDATION</th>\n",
       "      <th>ORDERED_QUANTITY</th>\n",
       "      <th>MONTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>YOMIURI GIANTS</td>\n",
       "      <td>LP5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Custom</td>\n",
       "      <td>72</td>\n",
       "      <td>2023-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>Japan</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>YOMIURI GIANTS</td>\n",
       "      <td>LP5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Custom</td>\n",
       "      <td>72</td>\n",
       "      <td>2023-10-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emerging Markets</td>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>LAS VEGAS RAIDERS</td>\n",
       "      <td>940</td>\n",
       "      <td>FOOTBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Program</td>\n",
       "      <td>80</td>\n",
       "      <td>2023-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>RANCHO CUCAMONGA QUAKES</td>\n",
       "      <td>5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Program</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>North America</td>\n",
       "      <td>United States</td>\n",
       "      <td>Wovens</td>\n",
       "      <td>RENO ACES</td>\n",
       "      <td>5950</td>\n",
       "      <td>BASEBALL</td>\n",
       "      <td>Headwear</td>\n",
       "      <td>Program</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-11-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             REGION SALES_ORG_NAME FABRIC_TYPE                     TEAM  \\\n",
       "0             Japan          Japan      Wovens           YOMIURI GIANTS   \n",
       "1             Japan          Japan      Wovens           YOMIURI GIANTS   \n",
       "2  Emerging Markets    New Zealand      Wovens        LAS VEGAS RAIDERS   \n",
       "3     North America  United States      Wovens  RANCHO CUCAMONGA QUAKES   \n",
       "4     North America  United States      Wovens                RENO ACES   \n",
       "\n",
       "  SILHOUETTE     SPORT DIVISION_NAME SEASON_CONSOLIDATION  ORDERED_QUANTITY  \\\n",
       "0     LP5950  BASEBALL      Headwear               Custom                72   \n",
       "1     LP5950  BASEBALL      Headwear               Custom                72   \n",
       "2        940  FOOTBALL      Headwear              Program                80   \n",
       "3       5950  BASEBALL      Headwear              Program                 1   \n",
       "4       5950  BASEBALL      Headwear              Program                 1   \n",
       "\n",
       "       MONTH  \n",
       "0 2023-10-01  \n",
       "1 2023-10-01  \n",
       "2 2023-11-01  \n",
       "3 2023-11-01  \n",
       "4 2023-11-01  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "fe9b6cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouped: (16220, 14)\n"
     ]
    }
   ],
   "source": [
    "group_keys = [\"SALES_ORG_NAME\", \"SILHOUETTE\", \"SEASON_CONSOLIDATION\", \"MONTH\"]\n",
    "\n",
    "df_grouped = (\n",
    "    df_main.groupby(group_keys)\n",
    "        .agg(\n",
    "            ORDERED_QUANTITY=('ORDERED_QUANTITY', 'sum'),\n",
    "\n",
    "            TEAM_COUNT=('TEAM', pd.Series.nunique),\n",
    "            SPORT_COUNT=('SPORT', pd.Series.nunique),\n",
    "            FABRIC_COUNT=('FABRIC_TYPE', pd.Series.nunique),\n",
    "            DIVISION_COUNT=('DIVISION_NAME', pd.Series.nunique),\n",
    "\n",
    "            REGION=('REGION', lambda x: x.iloc[0]),\n",
    "            TEAM=('TEAM', lambda x: x.iloc[0]),\n",
    "            SPORT=('SPORT', lambda x: x.iloc[0]),\n",
    "            FABRIC_TYPE=('FABRIC_TYPE', lambda x: x.iloc[0]),\n",
    "            DIVISION_NAME=('DIVISION_NAME', lambda x: x.iloc[0]),\n",
    "        )\n",
    "        .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Grouped:\", df_grouped.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1ff4598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fill: (27707, 14)\n"
     ]
    }
   ],
   "source": [
    "def fill_missing_months(df, keys):\n",
    "    out = []\n",
    "\n",
    "    for name, g in df.groupby(keys):\n",
    "        g = g.sort_values(\"MONTH\")\n",
    "\n",
    "        full_range = pd.date_range(\n",
    "            start=g[\"MONTH\"].min(),\n",
    "            end=g[\"MONTH\"].max(),\n",
    "            freq=\"MS\"\n",
    "        )\n",
    "\n",
    "        g2 = g.set_index(\"MONTH\").reindex(full_range)\n",
    "        g2.index.name = \"MONTH\"\n",
    "\n",
    "        num_cols = [\"ORDERED_QUANTITY\",\"TEAM_COUNT\",\"SPORT_COUNT\",\"FABRIC_COUNT\",\"DIVISION_COUNT\"]\n",
    "        g2[num_cols] = g2[num_cols].fillna(0)\n",
    "\n",
    "        cat_cols = [\"REGION\",\"TEAM\",\"SPORT\",\"FABRIC_TYPE\",\"DIVISION_NAME\"]\n",
    "        for c in cat_cols:\n",
    "            g2[c] = g2[c].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "        for i, k in enumerate(keys):\n",
    "            g2[k] = name[i]\n",
    "\n",
    "        out.append(g2.reset_index())\n",
    "\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "df_filled = fill_missing_months(\n",
    "    df_grouped,\n",
    "    keys=[\"SALES_ORG_NAME\", \"SILHOUETTE\", \"SEASON_CONSOLIDATION\"]\n",
    ")\n",
    "\n",
    "print(\"After fill:\", df_filled.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a35f5ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled[\"year\"] = df_filled[\"MONTH\"].dt.year\n",
    "df_filled[\"month_num\"] = df_filled[\"MONTH\"].dt.month\n",
    "df_filled[\"quarter\"] = df_filled[\"MONTH\"].dt.quarter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9156f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = df_filled.sort_values([\"SALES_ORG_NAME\",\"SILHOUETTE\",\"SEASON_CONSOLIDATION\",\"MONTH\"])\n",
    "\n",
    "df_filled[\"time_idx\"] = (\n",
    "    df_filled[\"MONTH\"].dt.year * 12 + df_filled[\"MONTH\"].dt.month\n",
    ")\n",
    "\n",
    "df_filled[\"time_idx\"] = df_filled[\"time_idx\"] - df_filled[\"time_idx\"].min()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "50dfe18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    \"SALES_ORG_NAME\", \"SILHOUETTE\", \"SEASON_CONSOLIDATION\",\n",
    "    \"REGION\", \"SPORT\", \"TEAM\", \"FABRIC_TYPE\", \"DIVISION_NAME\"\n",
    "]\n",
    "\n",
    "for c in categorical_cols:\n",
    "    df_filled[c] = df_filled[c].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e1b7eed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24143, 18) (3564, 18)\n"
     ]
    }
   ],
   "source": [
    "test_start = pd.Timestamp(\"2024-01-01\")\n",
    "\n",
    "train_df = df_filled[df_filled[\"MONTH\"] < test_start].copy()\n",
    "test_df  = df_filled[df_filled[\"MONTH\"] >= test_start].copy()\n",
    "\n",
    "print(train_df.shape, test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dd54bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"SALES_ORG_NAME\", \"SILHOUETTE\", \"SEASON_CONSOLIDATION\"]\n",
    "\n",
    "static_categoricals = group_cols\n",
    "\n",
    "time_varying_known_categoricals = [\"REGION\",\"FABRIC_TYPE\",\"SPORT\",\"TEAM\",\"DIVISION_NAME\"]\n",
    "time_varying_known_reals = [\"month_num\", \"quarter\"]\n",
    "\n",
    "time_varying_unknown_reals = [\n",
    "    \"ORDERED_QUANTITY\",\n",
    "    \"TEAM_COUNT\",\"SPORT_COUNT\",\"FABRIC_COUNT\",\"DIVISION_COUNT\"\n",
    "]\n",
    "\n",
    "target = \"ORDERED_QUANTITY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d56aab80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         SALES_ORG_NAME SILHOUETTE SEASON_CONSOLIDATION  start    end  length\n",
      "188         South Korea        920                Stock   95.0   95.0     1.0\n",
      "17            Australia     LP5950                Stock   87.0   87.0     1.0\n",
      "87               German     LP5950               Custom  180.0  180.0     1.0\n",
      "159  NEC China Shanghai     LP5950               Custom  181.0  181.0     1.0\n",
      "231           US Retail     LP5950               Custom  133.0  136.0     4.0\n",
      "143              Mexico     LP5950                Stock   95.0  100.0     6.0\n",
      "216           US Retail       3930               Custom  133.0  139.0     7.0\n",
      "37                China       3930              Program   90.0   99.0    10.0\n",
      "92                Japan       3930                Stock  194.0  206.0    13.0\n",
      "83               German        940                Stock  193.0  205.0    13.0\n",
      "76               German       5950              Program  191.0  205.0    15.0\n",
      "50                China        950                Stock   96.0  111.0    16.0\n",
      "228           US Retail        950               Custom  132.0  148.0    17.0\n",
      "41                China       5950                Stock   90.0  108.0    19.0\n",
      "73               German       3930              Program  186.0  204.0    19.0\n",
      "\n",
      "Minimum length across all series: 1.0\n",
      "Series with < 48 months: 28\n"
     ]
    }
   ],
   "source": [
    "check = (\n",
    "    df_filled.groupby([\"SALES_ORG_NAME\",\"SILHOUETTE\",\"SEASON_CONSOLIDATION\"])\n",
    "    .agg(\n",
    "        start=(\"time_idx\",\"min\"),\n",
    "        end=(\"time_idx\",\"max\"),\n",
    "        length=(\"time_idx\", lambda x: x.max() - x.min() + 1)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"length\")\n",
    ")\n",
    "\n",
    "print(check.head(15))  # the shortest series\n",
    "print(\"\\nMinimum length across all series:\", check[\"length\"].min())\n",
    "print(\"Series with < 48 months:\", (check[\"length\"] < 48).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "020e7ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining rows: (27099, 18)\n",
      "Remaining unique series: 186\n"
     ]
    }
   ],
   "source": [
    "# Remove all groups shorter than 48 months\n",
    "valid_groups = check[check[\"length\"] >= 48][\n",
    "    [\"SALES_ORG_NAME\",\"SILHOUETTE\",\"SEASON_CONSOLIDATION\"]\n",
    "]\n",
    "\n",
    "df_filtered = df_filled.merge(valid_groups, on=[\"SALES_ORG_NAME\",\"SILHOUETTE\",\"SEASON_CONSOLIDATION\"], how=\"inner\")\n",
    "\n",
    "print(\"Remaining rows:\", df_filtered.shape)\n",
    "print(\"Remaining unique series:\", df_filtered.groupby([\"SALES_ORG_NAME\",\"SILHOUETTE\",\"SEASON_CONSOLIDATION\"]).ngroups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f2bbd4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_forecasting/data/timeseries.py:1282: UserWarning: Min encoder length and/or min_prediction_idx and/or min prediction length and/or lags are too large for 37 series/groups which therefore are not present in the dataset index. This means no predictions can be made for those series. First 10 removed groups: [{'__group_id__SALES_ORG_NAME': 'Australia', '__group_id__SILHOUETTE': '3930', '__group_id__SEASON_CONSOLIDATION': 'Stock'}, {'__group_id__SALES_ORG_NAME': 'Australia', '__group_id__SILHOUETTE': 'LP5950', '__group_id__SEASON_CONSOLIDATION': 'Stock'}, {'__group_id__SALES_ORG_NAME': 'China', '__group_id__SILHOUETTE': '3930', '__group_id__SEASON_CONSOLIDATION': 'Program'}, {'__group_id__SALES_ORG_NAME': 'China', '__group_id__SILHOUETTE': '5950', '__group_id__SEASON_CONSOLIDATION': 'Stock'}, {'__group_id__SALES_ORG_NAME': 'China', '__group_id__SILHOUETTE': '950', '__group_id__SEASON_CONSOLIDATION': 'Stock'}, {'__group_id__SALES_ORG_NAME': 'German', '__group_id__SILHOUETTE': '3930', '__group_id__SEASON_CONSOLIDATION': 'Custom'}, {'__group_id__SALES_ORG_NAME': 'German', '__group_id__SILHOUETTE': '920', '__group_id__SEASON_CONSOLIDATION': 'Custom'}, {'__group_id__SALES_ORG_NAME': 'German', '__group_id__SILHOUETTE': '920', '__group_id__SEASON_CONSOLIDATION': 'Program'}, {'__group_id__SALES_ORG_NAME': 'German', '__group_id__SILHOUETTE': '940', '__group_id__SEASON_CONSOLIDATION': 'Custom'}, {'__group_id__SALES_ORG_NAME': 'German', '__group_id__SILHOUETTE': '940', '__group_id__SEASON_CONSOLIDATION': 'Program'}]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "max_encoder_length = 36\n",
    "max_prediction_length = 12\n",
    "\n",
    "training_dataset = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target,\n",
    "    group_ids=group_cols,\n",
    "\n",
    "    static_categoricals=static_categoricals,\n",
    "\n",
    "    time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "\n",
    "    target_normalizer=None,\n",
    "\n",
    "    categorical_encoders={\n",
    "        col: NaNLabelEncoder(add_nan=True)\n",
    "        for col in (static_categoricals + time_varying_known_categoricals)\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c9bca567",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [\"SILHOUETTE\", \"SEASON_CONSOLIDATION\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "dfa25d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\n",
    "    \"SALES_ORG_NAME\", \"SILHOUETTE\", \"SEASON_CONSOLIDATION\",\n",
    "    \"REGION\", \"SPORT\", \"TEAM\", \"FABRIC_TYPE\", \"DIVISION_NAME\"\n",
    "]\n",
    "\n",
    "for c in categorical_cols:\n",
    "    df_filled[c] = df_filled[c].astype(\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "316959e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = pd.Timestamp(\"2024-01-01\")\n",
    "\n",
    "train_df = df_filtered[df_filtered[\"MONTH\"] < test_start].copy()\n",
    "test_df  = df_filtered[df_filtered[\"MONTH\"] >= test_start].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4b4dc564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_id used by TFT\n",
    "group_cols = [\"SILHOUETTE\", \"SEASON_CONSOLIDATION\"]\n",
    "\n",
    "# static (one value per series)\n",
    "static_categoricals = [\"SILHOUETTE\", \"SEASON_CONSOLIDATION\"]\n",
    "\n",
    "# known categorical features at future time\n",
    "time_varying_known_categoricals = [\n",
    "    \"SALES_ORG_NAME\", \"REGION\", \"FABRIC_TYPE\", \"SPORT\", \"DIVISION_NAME\"\n",
    "]\n",
    "\n",
    "# known numeric time features (can be forecasted into future)\n",
    "time_varying_known_reals = [\"month_num\", \"quarter\"]\n",
    "\n",
    "# unknown numeric features (only available historically)\n",
    "time_varying_unknown_reals = [\n",
    "    \"ORDERED_QUANTITY\",\n",
    "    \"TEAM_COUNT\", \"SPORT_COUNT\", \"FABRIC_COUNT\", \"DIVISION_COUNT\"\n",
    "]\n",
    "\n",
    "target = \"ORDERED_QUANTITY\"\n",
    "max_encoder_length = 36\n",
    "max_prediction_length = 12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b3fb50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import NaNLabelEncoder\n",
    "\n",
    "training_dataset = TimeSeriesDataSet(\n",
    "    df_filtered,                       # use full filtered dataset\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"ORDERED_QUANTITY\",\n",
    "    group_ids=[\"SILHOUETTE\", \"SEASON_CONSOLIDATION\"],\n",
    "\n",
    "    # categorical groups\n",
    "    static_categoricals=[\"SILHOUETTE\", \"SEASON_CONSOLIDATION\"],\n",
    "    time_varying_known_categoricals=[\"SALES_ORG_NAME\", \"REGION\", \"FABRIC_TYPE\", \"SPORT\", \"DIVISION_NAME\"],\n",
    "\n",
    "    # continuous features\n",
    "    time_varying_known_reals=[\"month_num\", \"quarter\"],\n",
    "    time_varying_unknown_reals=[\"ORDERED_QUANTITY\", \"TEAM_COUNT\", \"SPORT_COUNT\", \"FABRIC_COUNT\", \"DIVISION_COUNT\"],\n",
    "\n",
    "    max_encoder_length=36,\n",
    "    max_prediction_length=12,\n",
    "\n",
    "    target_normalizer=None,\n",
    "\n",
    "    categorical_encoders={\n",
    "        col: NaNLabelEncoder(add_nan=True)\n",
    "        for col in (\n",
    "            [\"SILHOUETTE\", \"SEASON_CONSOLIDATION\"]\n",
    "            + [\"SALES_ORG_NAME\", \"REGION\", \"FABRIC_TYPE\", \"SPORT\", \"DIVISION_NAME\"]\n",
    "        )\n",
    "    },\n",
    "\n",
    "    allow_missing_timesteps=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3686d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    training_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "acf08fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 577744747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”’ USING TRAINING SEED: 577744747\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "def generate_and_set_seed(seed=None):\n",
    "    if seed is None:\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "\n",
    "    print(f\"\\nðŸ”’ USING TRAINING SEED: {seed}\\n\")\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    pl.seed_everything(seed, workers=True)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    return seed\n",
    "\n",
    "seed = generate_and_set_seed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "178a3aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6b27a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training_dataset,\n",
    "    learning_rate=0.003,\n",
    "    hidden_size=32,               # REQUIRED\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "\n",
    "    loss=QuantileLoss([0.1, 0.5, 0.9]),\n",
    "\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "10293409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor=\"train_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=5,\n",
    "    verbose=True,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    dirpath=\"tft_checkpoints/\",\n",
    "    filename=\"tft_best\",\n",
    "    save_top_k=1,\n",
    "    monitor=\"train_loss\",\n",
    "    mode=\"min\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "185c5a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger(\"tft_logs\", name=\"tft\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=40,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    gradient_clip_val=0.1,\n",
    "    callbacks=[early_stop, checkpoint],\n",
    "    logger=csv_logger,      # <--- IMPORTANT\n",
    "    log_every_n_steps=20\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5d963867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:108: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 449   \n",
      "3  | prescalers                         | ModuleDict                      | 224   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 238   \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 15.8 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 5.0 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.6 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K \n",
      "20 | output_layer                       | Linear                          | 99    \n",
      "----------------------------------------------------------------------------------------\n",
      "74.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "74.3 K    Total params\n",
      "0.297     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 0/305 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 128, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 128, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:187\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    186\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 187\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     batch_idx, batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetching_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m start_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_start()\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py:571\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m        a collections of batch data\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py:583\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    575\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \n\u001b[1;32m    577\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m        Any: a collections of batch data\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py:66\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# fast path for the most common cases:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype):  \u001b[38;5;66;03m# single element\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [function(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 61, in fetch\n    return self.collate_fn(data)\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 265, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 143, in collate\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 143, in <listcomp>\n    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 128, in collate\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 128, in <dictcomp>\n    return elem_type({key: collate([d[key] for d in batch], collate_fn_map=collate_fn_map) for key in elem})\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 120, in collate\n    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)\n  File \"/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py\", line 162, in collate_tensor_fn\n    out = elem.new(storage).resize_(len(batch), *list(elem.size()))\nRuntimeError: Trying to resize storage that is not resizable\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3e584",
   "metadata": {},
   "source": [
    "# Model TFT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "f4ac8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "6c6a3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_main.copy()\n",
    "\n",
    "# Ensure MONTH is datetime monthly timestamp\n",
    "df[\"MONTH\"] = pd.to_datetime(df[\"MONTH\"])\n",
    "\n",
    "# Create integer time index\n",
    "df = df.sort_values(\"MONTH\")\n",
    "df[\"time_idx\"] = (df[\"MONTH\"] - df[\"MONTH\"].min()).dt.days // 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "acadc7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = [\"SILHOUETTE\", \"SEASON_CONSOLIDATION\"]\n",
    "group_cols = [\"SILHOUETTE\", \"SEASON_CONSOLIDATION\"]\n",
    "target = \"ORDERED_QUANTITY\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6207291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_categoricals = [\"SILHOUETTE\", \"SEASON_CONSOLIDATION\"]\n",
    "time_varying_known_categoricals = [\"REGION\", \"SALES_ORG_NAME\", \"FABRIC_TYPE\", \"TEAM\", \"SPORT\", \"DIVISION_NAME\"]\n",
    "time_varying_known_reals = [\"time_idx\"]\n",
    "time_varying_unknown_reals = [\"ORDERED_QUANTITY\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "47099d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid groups: 18\n",
      "Remaining rows: (6080224, 11)\n"
     ]
    }
   ],
   "source": [
    "min_length = 36 + 12  # 48\n",
    "\n",
    "# Count months per group\n",
    "group_lengths = df.groupby(group_cols)[\"time_idx\"].nunique().reset_index(name=\"length\")\n",
    "\n",
    "# Keep only groups >= 48 months\n",
    "valid_groups = group_lengths[group_lengths[\"length\"] >= min_length][group_cols]\n",
    "\n",
    "# Filter main dataframe\n",
    "df = df.merge(valid_groups, on=group_cols, how=\"inner\")\n",
    "\n",
    "print(\"Valid groups:\", valid_groups.shape[0])\n",
    "print(\"Remaining rows:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3e6e8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time_idx = df[\"time_idx\"].max()\n",
    "\n",
    "train_df = df[df[\"time_idx\"] <= max_time_idx - 12]\n",
    "test_df  = df[df[\"time_idx\"] >  max_time_idx - 12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "11e957a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TimeSeriesDataSet(\n",
    "    train_df,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=target,\n",
    "    group_ids=group_cols,\n",
    "\n",
    "    # Static features\n",
    "    static_categoricals=static_categoricals,\n",
    "\n",
    "    # Known (future available)\n",
    "    time_varying_known_categoricals=time_varying_known_categoricals,\n",
    "    time_varying_known_reals=time_varying_known_reals,\n",
    "\n",
    "    # Unknown (observed only in encoder)\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "\n",
    "    max_encoder_length=36,\n",
    "    max_prediction_length=12,\n",
    "\n",
    "    target_normalizer=None,\n",
    "    \n",
    "    categorical_encoders={\n",
    "        col: NaNLabelEncoder(add_nan=True)\n",
    "        for col in (static_categoricals + time_varying_known_categoricals)\n",
    "    },\n",
    "\n",
    "    allow_missing_timesteps=True\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "75df34db",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = training_dataset.to_dataloader(\n",
    "    train=True,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "31855cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training_dataset,\n",
    "    learning_rate=0.003,\n",
    "    hidden_size=32,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=16,\n",
    "\n",
    "    loss=QuantileLoss([0.1, 0.5, 0.9]),\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "402a8ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "logger = CSVLogger(\"tft_logs\", name=\"tft_model\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=30,\n",
    "    accelerator=\"cpu\",   # Use \"gpu\" if available\n",
    "    logger=logger,\n",
    "    enable_checkpointing=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b857e292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 112 K \n",
      "3  | prescalers                         | ModuleDict                      | 64    \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 238   \n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 5.5 K \n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 3.4 K \n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 4.3 K \n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 4.3 K \n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 4.3 K \n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 4.3 K \n",
      "11 | lstm_encoder                       | LSTM                            | 8.4 K \n",
      "12 | lstm_decoder                       | LSTM                            | 8.4 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 2.1 K \n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 64    \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 5.3 K \n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 2.6 K \n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 2.2 K \n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 4.3 K \n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 2.2 K \n",
      "20 | output_layer                       | Linear                          | 99    \n",
      "----------------------------------------------------------------------------------------\n",
      "174 K     Trainable params\n",
      "0         Non-trainable params\n",
      "174 K     Total params\n",
      "0.699     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | 0/53713 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).\n",
      "\u0000"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 470221) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1120\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/multiprocessing/queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[0;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:608\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    606\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_unwrap_optimized(model)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 608\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[38;5;241m.\u001b[39m_call_teardown_hook()\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:650\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    643\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    646\u001b[0m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    648\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    649\u001b[0m )\n\u001b[0;32m--> 650\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1112\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39mresume_end()\n\u001b[0;32m-> 1112\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m log\u001b[38;5;241m.\u001b[39mdetail(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1191\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1214\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1214\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py:267\u001b[0m, in \u001b[0;36mFitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher\u001b[38;5;241m.\u001b[39msetup(dataloader, batch_to_device\u001b[38;5;241m=\u001b[39mbatch_to_device)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_training_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/loops/loop.py:199\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py:187\u001b[0m, in \u001b[0;36mTrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_fetcher, DataLoaderIterDataFetcher):\n\u001b[1;32m    186\u001b[0m     batch_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 187\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m     batch_idx, batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(data_fetcher)\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:184\u001b[0m, in \u001b[0;36mAbstractDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetching_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:265\u001b[0m, in \u001b[0;36mDataFetcher.fetching_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 265\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# consume the batch we just fetched\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/utilities/fetching.py:280\u001b[0m, in \u001b[0;36mDataFetcher._fetch_next_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m start_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_fetch_start()\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 280\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_profiler()\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py:571\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    566\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fetches the next batch from multiple data loaders.\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;124;03m        a collections of batch data\u001b[39;00m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_next_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader_iters\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/pytorch_lightning/trainer/supporters.py:583\u001b[0m, in \u001b[0;36mCombinedLoaderIterator.request_next_batch\u001b[0;34m(loader_iters)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrequest_next_batch\u001b[39m(loader_iters: Union[Iterator, Sequence, Mapping]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    575\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the batch of data from multiple iterators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \n\u001b[1;32m    577\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;124;03m        Any: a collections of batch data\u001b[39;00m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_to_collection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/lightning_utilities/core/apply_func.py:66\u001b[0m, in \u001b[0;36mapply_to_collection\u001b[0;34m(data, dtype, function, wrong_dtype, include_none, allow_frozen, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# fast path for the most common cases:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, dtype):  \u001b[38;5;66;03m# single element\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mlist\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, dtype) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data):  \u001b[38;5;66;03m# 1d homogeneous list\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [function(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1315\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1316\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1282\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1284\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/anaconda/envs/nec10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1132\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pids_str)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 470221) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast_group(silhouette, season):\n",
    "    df_group = df[df[\"SILHOUETTE\"] == silhouette]\n",
    "    df_group = df_group[df_group[\"SEASON_CONSOLIDATION\"] == season]\n",
    "\n",
    "    # Build a new prediction dataset\n",
    "    pred_dataset = TimeSeriesDataSet.from_dataset(\n",
    "        training_dataset,\n",
    "        df_group,\n",
    "        stop_randomization=True,\n",
    "        predict=True,\n",
    "    )\n",
    "\n",
    "    pred_loader = pred_dataset.to_dataloader(train=False, batch_size=64)\n",
    "\n",
    "    raw_predictions, x = tft.predict(pred_loader, mode=\"raw\", return_x=True)\n",
    "\n",
    "    return raw_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b117a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = forecast_group(\"5950\", \"Stock\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nec10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
